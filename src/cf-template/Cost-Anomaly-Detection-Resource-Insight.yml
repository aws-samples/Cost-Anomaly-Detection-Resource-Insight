AWSTemplateFormatVersion: '2010-09-09'
Description: AWS CADRI - Enhance the event from AWS Cost Anomaly Detection with information from the top resource IDs that contributed to the anomaly.
Parameters:
  DefaultNoticationFlow:
    Type: String
    Default: 'yes'
    AllowedValues: ['yes', 'no']
    Description: 'Would you like to deploy the default notification flow that uses SNS to send the enhanced Cost Anomaly Detection messages?'
  Email:
    Type: String
    AllowedPattern: '[^@]+@[^@]+\.[^@]+'
    Description: 'Email address to be associated with the SNS topic for the default notification flow'
    ConstraintDescription: 'Please ensure that the provided email address is valid and correctly formatted'
  AthenaDB:
    Type: String
    Description: 'Database that contains the table with the Cost and Usage report'
  AthenaTable:
    Type: String
    Description: 'Name of the table with the Cost and Usage report'
  QueryOutputLocation:
    Type: String
    Description: 'S3 location to store Athena query results. Only the name without S3://'
  CURS3Bucket:
    Type: String
    Description: 'S3 location where the Cost and Usage Report. Only the name without S3://'
  ConcurrencyLimit:
    Type: Number
    Default: 1
    Description: 'Enter the concurrency limit for the Lambda functions (default is 1).'

Conditions:
  ShouldDeployDefaultNotificationFlowResources:
    !Equals [!Ref DefaultNoticationFlow, "yes"]

Resources:
  EventBridgeBus:
    Type: AWS::Events::EventBus
    Properties:
      Name: !Sub ${AWS::StackName}-CADRI-EventBridge-bus
  
  EventsSNSTopic:
    Type: AWS::SNS::Topic
    Properties:
      KmsMasterKeyId: alias/aws/sns
      DisplayName: !Sub ${AWS::StackName}-CADRI-Event-topic
      TopicName: !Sub ${AWS::StackName}-CADRI-Event-topic
  
  EventsSNSPolicy:
    Type: AWS::SNS::TopicPolicy
    Properties:
      Topics:
        - !Ref EventsSNSTopic
      PolicyDocument:
        Version: "2008-10-17"
        Id: "AWS Cost Anomaly Detection Policy"
        Statement:
          - Sid: "AWSAnomalyDetectionSNSPublishingPermissions"
            Effect: "Allow"
            Principal:
              Service: "costalerts.amazonaws.com"
            Action: "SNS:Publish"
            Resource: !Ref EventsSNSTopic
            Condition:
              StringEquals:
                aws:SourceAccount: !Ref "AWS::AccountId"

  LambdaEnhanceCostAnomalyDetectionFunction:
    Type: AWS::Lambda::Function
    # Test case for check skip via comment
    # checkov:skip=CKV_AWS_116: "Ensure that AWS Lambda function is configured for a Dead Letter Queue(DLQ)"
    # checkov:skip=CKV_AWS_173: "Check encryption settings for Lambda environment variable"
    # checkov:skip=CKV_AWS_117: "Ensure that AWS Lambda function is configured inside a VPC"
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W58
            reason: "Permissions granted, CFN_Nag not parsing correctly?"
          - id: W89
            reason: "Not applicable for use case"
    Properties:
      Description: 'AWS CADRI - Lambda function that receives the event from AWS Cost Anomaly Detection and enhances the notification with the main resource IDs.'
      FunctionName: !Sub ${AWS::StackName}-CADRI-enhance-event
      Handler: index.lambda_handler
      Runtime: python3.13
      Role: !GetAtt 'LambdaEnhanceCostAnomalyRole.Arn'
      MemorySize: 128
      Timeout: 480
      ReservedConcurrentExecutions: !Ref ConcurrencyLimit
      Environment:
        Variables:
          ATHENA_DATABSE: !Ref AthenaDB
          ATHENA_OUTPUT_LOCATION: !Ref QueryOutputLocation
          ATHENA_TABLE: !Ref AthenaTable
          EVENT_BRIDGE_BUS_NAME: !Ref EventBridgeBus
          EVENT_BRIDGE_DETAIL_TYPE: 'CADRIEvent'
          EVENT_BRIDGE_SOURCE_NAME: 'custom.cadri'
          LOG_LEVEL: 'DEBUG'
      Code:
        ZipFile: |
          import os
          import json
          import logging
          from datetime import datetime, timedelta
          import boto3
          import time
          import traceback

          logger = logging.getLogger(__name__)
          logger.setLevel(getattr(logging, os.environ.get('LOG_LEVEL', 'INFO').upper(), logging.INFO))
          #logging.getLogger().setLevel(logging.DEBUG)

          def lambda_handler(event, context):
              logger.debug(f"Incoming event: {json.dumps(event)}")
              
              if not event.get('Records'):
                      logger.error("No Records found in event")
                      return {
                          'statusCode': 500,
                          'body': 'No Records found in event'
                      }
              processed_records = 0
              failed_records = 0
              for record in event['Records']:
                  try:
                      response, data = process_message_for_athena(record)
                      logger.debug(f"reponse type {type(response)}")
                      
                      # Ensure response is a dictionary
                      response_json = {
                          "anomalies": response if isinstance(response, list) else [response],
                          "anomaly_count": len(response) if isinstance(response, list) else 1
                      }

                      #response_json=json.loads(json.dumps(response))
                      logger.debug(f"response_json type {type(response_json)}")

                      table = format_data_as_table(data)
                      email_table = {
                          "email_table": table
                      }
                      response_json.update(email_table)
                      original_alert = json.loads(f'{{ "original_alert": {record["Sns"]["Message"]} }}')
                      logger.debug(f"original_alert type {type(original_alert)}")
                      response_json.update(original_alert)
                      logger.debug(f"json after merging: {json.dumps(response_json)}")
                      eb_result = post_to_eventbridge(response_json)
                      logger.debug(f"eb_result: {json.dumps(eb_result)}")
                      processed_records += 1
                  except Exception as e:
                          logger.error(f"Error processing record: {str(e)}")
                          logger.error(f"Failed record: {json.dumps(record)}")
                          logger.error(traceback.format_exc())
                          failed_records += 1

              logger.info(f"Processed {processed_records} records successfully. Failed to process {failed_records} records.")

              return {
                  'statusCode': 200,
                  'body': json.dumps({
                      'processed_records': processed_records,
                      'failed_records': failed_records
                  })
              }

          def post_to_eventbridge(event_detail):
              event_bus_source = os.environ.get('EVENT_BRIDGE_SOURCE_NAME')
              event_detail_type = os.environ.get('EVENT_BRIDGE_DETAIL_TYPE')
              event_bus_name = os.environ.get('EVENT_BRIDGE_BUS_NAME')
              if not event_bus_name:
                  raise Exception("EVENT_BRIDGE_BUS_NAME environment variables not set.")
              
              if not event_detail_type:
                  raise Exception("EVENT_BRIDGE_DETAIL_TYPE environment variables not set.") 
              if not event_bus_source:
                  raise Exception("EVENT_BRIDGE_SOURCE_NAME environment variables not set.") 
              
              eventbridge = boto3.client('events')
              try:
                  response = eventbridge.put_events(
                      Entries=[
                          {
                              'Source': event_bus_source,
                              'DetailType': event_detail_type,
                              'Detail': json.dumps(event_detail),
                              'EventBusName': event_bus_name  
                          }
                      ]
                  )
                  
                  return {
                      'statusCode': 200,
                      'body': json.dumps({
                          'message': 'Event published successfully',
                          'eventID': response['Entries'][0]['EventId']
                      })
                  }
                  
              except Exception as e:
                  logger.error(f"Error processing record: {str(e)}")
                  raise

          def process_message_for_athena(record):
              try:
                  message = json.loads(record['Sns']['Message'])
                  logger.info(f"Processed message {message}")
                  
                  # Create the filter condition
                  conditions = []
                  for cause in message['rootCauses']:
                      logger.debug(f"rootCauses - 1 {cause}")
                      condition = (f"line_item_usage_account_id = '{cause['linkedAccount']}' AND "
                              f"line_item_usage_type = '{cause['usageType']}' ")
                      conditions.append(condition)
                  logger.debug(f"conditions - 1 {conditions}")
                  # Join all conditions with ' OR '
                  account_service_and_usage_filter = ' OR '.join(conditions)

                  # Wrap the entire output in parentheses
                  account_service_and_usage_filter = f"({account_service_and_usage_filter})"

                  # Parse start and end dates from the event
                  start_date = datetime.strptime(message['anomalyStartDate'], "%Y-%m-%dT%H:%M:%SZ")
                  end_date = datetime.strptime(message['anomalyEndDate'], "%Y-%m-%dT%H:%M:%SZ")

                  logger.debug(f"start_date - {start_date} end_date {end_date}")
                  # Calculate the duration of the anomaly
                  duration = (end_date - start_date).days + 1

                  # Calculate date parameters for the query
                  query_start_date = start_date - timedelta(days=duration)
                  query_end_date = end_date + timedelta(days=1)
                  previous_period_start_date = start_date - timedelta(days=duration)
                  previous_period_end_date = end_date - timedelta(days=duration)

                  # Format the dates as strings for the SQL query
                  query_start_date_str = query_start_date.strftime('%Y-%m-%d')
                  query_end_date_str = query_end_date.strftime('%Y-%m-%d')
                  previous_period_start_date_str = previous_period_start_date.strftime('%Y-%m-%d')
                  previous_period_end_date_str = previous_period_end_date.strftime('%Y-%m-%d')
                  current_period_start_date_str = start_date.strftime('%Y-%m-%d')
                  current_period_end_date_str = end_date.strftime('%Y-%m-%d')

                  # Specify the Athena table name
                  table_name = os.environ.get('ATHENA_TABLE')
                  if not table_name:
                      raise Exception("ATHENA_TABLE environment variables not set.")
                  #table_name = 'cur'

                  athena_query = f"""
                      WITH daily_costs AS (
                          SELECT 
                              line_item_resource_id,
                              line_item_usage_account_id,
                              product_servicename,
                              DATE(line_item_usage_start_date) AS usage_date,
                              SUM(line_item_unblended_cost) AS total_cost
                          FROM 
                              "{table_name}"
                          WHERE 
                              {account_service_and_usage_filter}
                              AND line_item_usage_start_date >= DATE '{query_start_date_str}'
                              AND line_item_usage_start_date < DATE '{query_end_date_str}'
                          GROUP BY 
                              line_item_resource_id, 
                              line_item_usage_account_id,
                              product_servicename,
                              DATE(line_item_usage_start_date)
                      ),
                      cost_summary AS (
                          SELECT 
                              line_item_resource_id,
                              line_item_usage_account_id,
                              product_servicename,
                              SUM(CASE WHEN usage_date BETWEEN DATE '{current_period_start_date_str}' AND DATE '{current_period_end_date_str}' THEN total_cost ELSE 0 END) AS anomaly_period_cost,
                              SUM(CASE WHEN usage_date BETWEEN DATE '{previous_period_start_date_str}' AND DATE '{previous_period_end_date_str}' THEN total_cost ELSE 0 END) AS previous_period_cost
                          FROM 
                              daily_costs
                          GROUP BY 
                              line_item_resource_id,
                              line_item_usage_account_id,
                              product_servicename
                      ),
                      cost_growth AS (
                      SELECT 
                              line_item_usage_account_id,
                              product_servicename,
                              line_item_resource_id,
                              anomaly_period_cost,
                              previous_period_cost,
                              (anomaly_period_cost - previous_period_cost) AS cost_increase,
                              CASE 
                                  WHEN previous_period_cost = 0 THEN 100
                                  ELSE ((anomaly_period_cost - previous_period_cost) / previous_period_cost) * 100
                              END AS percentage_increase
                          FROM 
                              cost_summary
                      )
                      SELECT 
                          line_item_usage_account_id,
                          product_servicename,
                          line_item_resource_id,
                          anomaly_period_cost,
                          previous_period_cost,
                          cost_increase,
                          percentage_increase
                      FROM 
                          cost_growth
                      WHERE
                          cost_increase > 0
                      ORDER BY 
                          cost_increase DESC
                      LIMIT 5;
                  """
                  logger.debug(f"Generated Athena query {athena_query}")
                  results, data = run_athena_query(athena_query)
                  logger.debug(f"Athena results {json.dumps(results)}")    
                  return results, data
              except Exception as e:
                  logger.error(f"Error processing Athena message : {str(e)}")
                  logger.error(traceback.format_exc())
                  raise
              
          def run_athena_query(query_id):
              """Return answer to Bedrock Agent in expected format."""
              try:
                  # Extract parameters from the event
                  database = os.environ.get('ATHENA_DATABSE')
                  if not database:
                      raise Exception("ATHENA_DATABSE environment variables not set.")
                  output_s3_bucket = os.environ.get('ATHENA_OUTPUT_LOCATION')
                  if not output_s3_bucket:
                      raise Exception("ATHENA_OUTPUT_LOCATION environment variables not set.")
                  
                  output_location = f"s3://{output_s3_bucket}/"
                  logger.debug(f'{query_id=}')
                  
                  # Initialize Athena client
                  athena_client = boto3.client('athena')
                  response = athena_client.start_query_execution(
                      QueryString=query_id,
                      QueryExecutionContext={
                          'Database': database
                      },
                      ResultConfiguration={
                          'OutputLocation': output_location,
                      }
                  )
                  query_execution_id = response['QueryExecutionId']
                  
                  # Wait for the query to complete
                  while True:
                      query_status = athena_client.get_query_execution(QueryExecutionId=query_execution_id)
                      status = query_status['QueryExecution']['Status']['State']
                      
                      if status in ['SUCCEEDED', 'FAILED', 'CANCELLED']:
                          break
                      
                      time.sleep(1)
                  logger.debug(f"Status is  {status}")   
                  if status != 'SUCCEEDED':
                      error_message = query_status['QueryExecution']['Status'].get('AthenaError', 'Unknown error')
                      raise Exception(f"Athena query failed: {error_message}")

                  results = athena_client.get_query_results(QueryExecutionId=query_execution_id)
                  # Process the results as needed
                  rows = results['ResultSet']['Rows']
                  # First row contains column headers
                  headers = [col['VarCharValue'] for col in rows[0]['Data']]
                  
                  # Process data rows
                  data = []
                  for row in rows[1:]:
                      values = [field.get('VarCharValue', '') for field in row['Data']]
                      row_dict = dict(zip(headers, values))
                      data.append(row_dict)
                  logger.debug(f"data results --> {data}")
                  return data, rows
              except Exception as e:
                  logger.error(f"Error executing Athena query: {str(e)}")
                  logger.error(traceback.format_exc())
                  raise

          def format_data_as_table(data):
              try:
                  logger.debug(f"data in format_data_as_table {data}")
                  # Extract headers and rows from the data
                  headers = [item['VarCharValue'] for item in data[0]['Data']]
                  rows = [[entry['VarCharValue'] for entry in row['Data']] for row in data[1:]]

                  # Rename and order columns as required
                  column_names = ["Account id", "Service", "Resource id", "Current Cost", "Previous Cost", "% Growth"]
                  column_mapping = {
                      "Account id": "line_item_usage_account_id",
                      "Service": "product_servicename",
                      "Resource id": "line_item_resource_id",
                      "Current Cost": "anomaly_period_cost",
                      "Previous Cost": "previous_period_cost",
                      "% Growth": "percentage_increase",
                  }

                  mapped_rows = [
                      [
                          row[headers.index(column_mapping[column])]
                          if column_mapping[column] in headers
                          else ""
                          for column in column_names
                      ]
                      for row in rows
                  ]

                  # Adjust column widths based on the data
                  column_widths = [
                      max(len(str(item)) for item in col)
                      for col in zip(column_names, *mapped_rows)
                  ]

                  # Create the table separator
                  separator = "-" * (sum(column_widths) + len(column_widths) * 3 + 1)

                  # Format the header row
                  header_row = "| " + " | ".join(
                      column.ljust(width) for column, width in zip(column_names, column_widths)
                  ) + " |"

                  # Format the data rows
                  data_rows = [
                      "| " + " | ".join(
                          str(cell).ljust(width) for cell, width in zip(row, column_widths)
                      ) + " |"
                      for row in mapped_rows
                  ]

                  # Combine everything into the final table
                  table = "\n".join([separator, header_row, separator] + data_rows + [separator])
                  return table
              except Exception as e:
                  logger.error(traceback.format_exc())
                  raise
  LambdaEnhanceCostAnomalyRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Action:
              - 'sts:AssumeRole'
            Effect: 'Allow'
            Principal:
              Service:
                - 'lambda.amazonaws.com'
      Policies:
        - PolicyName: RolePolicy 
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                Resource: !Sub "arn:${AWS::Partition}:logs:${AWS::Region}:${AWS::AccountId}:*"
              - Effect: Allow
                Action:
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: !Sub "arn:${AWS::Partition}:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/lambda/${AWS::StackName}-CADRI-enhance-event:*"  
              - Effect: Allow
                Action:
                  - s3:ListBucket
                  - s3:PutObject
                  - s3:GetObject
                  - s3:GetBucketLocation
                Resource:
                  - !Sub "arn:${AWS::Partition}:s3:::${QueryOutputLocation}"
                  - !Sub "arn:${AWS::Partition}:s3:::${QueryOutputLocation}/*"
                  - !Sub "arn:${AWS::Partition}:s3:::${CURS3Bucket}"
                  - !Sub "arn:${AWS::Partition}:s3:::${CURS3Bucket}/*"
              - Effect: Allow
                Action:
                  - glue:GetDatabase
                  - glue:GetTable
                  - glue:GetPartitions
                Resource:
                  - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:catalog"
                  - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:database/${AthenaDB}"
                  - !Sub "arn:${AWS::Partition}:glue:${AWS::Region}:${AWS::AccountId}:table/${AthenaDB}/${AthenaTable}"
              - Effect: Allow
                Action:
                  - athena:StartQueryExecution
                  - athena:GetQueryExecution
                  - athena:GetQueryResults
                Resource:
                  - !Sub "arn:${AWS::Partition}:athena:${AWS::Region}:${AWS::AccountId}:workgroup/*"
              - Effect: Allow
                Action:
                  - events:PutEvents
                Resource: !GetAtt 'EventBridgeBus.Arn'
  LambdaSNSSubscription:
    Type: AWS::SNS::Subscription
    Properties:
      TopicArn: !Ref EventsSNSTopic
      Protocol: lambda
      Endpoint: !GetAtt LambdaEnhanceCostAnomalyDetectionFunction.Arn

  LambdaPermissionForSNS:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref LambdaEnhanceCostAnomalyDetectionFunction
      Action: 'lambda:InvokeFunction'
      Principal: 'sns.amazonaws.com'
      SourceArn: !Ref EventsSNSTopic

  # Default Notification flow

  NotificationSNSTopic:
    Type: AWS::SNS::Topic
    Condition: ShouldDeployDefaultNotificationFlowResources
    Properties:
      KmsMasterKeyId: alias/aws/sns
      DisplayName: !Sub ${AWS::StackName}-CADRI-notification-topic
      TopicName: !Sub ${AWS::StackName}-CADRI-notification-topic
      Subscription:
        - Endpoint: !Sub ${Email}
          Protocol: email
  
  LambdaSendNotificationFunction:
    Type: AWS::Lambda::Function
    Condition: ShouldDeployDefaultNotificationFlowResources
    # Test case for check skip via comment
    # checkov:skip=CKV_AWS_173: "Check encryption settings for Lambda environment variable"
    # checkov:skip=CKV_AWS_117: "Ensure that AWS Lambda function is configured inside a VPC"
    # checkov:skip=CKV_AWS_116: "Ensure that AWS Lambda function is configured for a Dead Letter Queue(DLQ)"
    Metadata:
      cfn_nag:
        rules_to_suppress:
          - id: W58
            reason: "Permissions granted, CFN_Nag not parsing correctly?"
          - id: W89
            reason: "Not applicable for use case"
    Properties:
      Description: 'AWS CADRI - Lambda function that sends the enhanced Cost Anomaly Detection notification.'
      FunctionName: !Sub ${AWS::StackName}-CADRI-send-notification
      Handler: index.lambda_handler
      Runtime: python3.13
      Role: !GetAtt 'LambdaSendNotificationRole.Arn'
      MemorySize: 128
      Timeout: 60
      ReservedConcurrentExecutions: !Ref ConcurrencyLimit
      Environment:
        Variables:
          SNS_TOPIC_ARN: !Ref NotificationSNSTopic
          LOG_LEVEL: 'DEBUG'
      Code:
        ZipFile: |
          import boto3
          import logging
          import os
          import traceback
          from botocore.config import Config


          logger = logging.getLogger(__name__)
          logger.setLevel(getattr(logging, os.environ.get('LOG_LEVEL', 'INFO').upper(), logging.INFO))
          #logging.getLogger().setLevel(logging.DEBUG)

          # Configure exponential backoff
          retry_config = Config(
              retries={
                  'max_attempts': 5,  # Total number of attempts
                  'mode': 'adaptive'  # AWS recommended adaptive retry mode
              }
          )

          def create_email_message(event):
              try:
                  # Extract anomalies
                  anomalies = event['detail']['anomalies']
                  logger.debug(f"Total number of anomalies: {event['detail']['anomaly_count']}\n")
                  logger.debug("Anomaly Details:")

                  mod_email_message = " "
                  total_cost_increase = 0.0
                  for idx, anomaly in enumerate(anomalies, 1):
                      message = f"""
                      {idx}, {anomaly['line_item_usage_account_id']}, {anomaly['line_item_resource_id']}, {anomaly['product_servicename']}, ${round(float(anomaly['anomaly_period_cost']),2)}, ${round(float(anomaly['previous_period_cost']),2)}, ${round(float(anomaly['cost_increase']),2)}, {round(float(anomaly['percentage_increase']),2)}%"""
                      total_cost_increase += float(anomaly['cost_increase'])
                      mod_email_message = mod_email_message + message
                      
                  summary = f"The above anomalies caused a total cost increase of ${round(total_cost_increase,2)}"

                  email_table = event['detail']['email_table']

                  anomaly_start_date = event['detail']['original_alert']['anomalyStartDate']
                  anomaly_end_date = event['detail']['original_alert']['anomalyEndDate']
                  original_anomaly_link = event['detail']['original_alert']['anomalyDetailsLink']
              except Exception as e:
                  logger.error('Error processing anomalies: ' + str(e))

              if not email_table:
                  email_table=f"Anomalies were generated but the systen is unable to get the details. Please review the Cost Anomaly Detection application for anomaly details"
              if not summary:
                  summary=f"Anomalies were generated but the systen is unable to get the details. Please review the Cost Anomaly Detection application for anomaly details"

              if not anomaly_start_date:
                  anomaly_start_date=f"UNAVAILABLE"
              if not anomaly_end_date:
                  anomaly_end_date=f"UNAVAILBLE"

              emailbody=f"""
              Hello,

              You are receiving this alert because AWS Cost Anomaly Detection has identified an unusual cost increase. 
              The anomaly has been validated and the root cause has been determined using the AWS Cost and Usage Report (CUR). 

              * Anomaly Start Date: {anomaly_start_date}
              * Anomaly End Date: {anomaly_end_date}

              Here is the list of the resources that triggered this cost anomaly:
              
              {email_table}
              
              Summary of the Alert: 
              
              {summary}
              
              Please verify if this cost increase is expected and, if necessary, make any adjustments.

              To view the original anomaly report, please click 
              {original_anomaly_link}

              Thank you,
              Anomaly Detection Agent 

              """
              logger.debug(f"emailbody is {emailbody}")
              return emailbody

          def lambda_handler(event, context):

              logger.debug(event)
              emailSnsTopic = os.environ.get('SNS_TOPIC_ARN')
              if not emailSnsTopic:
                  raise Exception("SNS_TOPIC_ARN environment variables not set.")
              emailSubject = 'AWS Cost Anomaly Detection Resource Insight Alert'
              emailMessage = create_email_message(event)
              
              if emailMessage is not None and emailSubject is not None and emailSnsTopic is not None:
                  sns = boto3.client('sns', config=retry_config)
                  params = {
                      'Message': emailMessage,
                      'Subject': emailSubject,
                      'TopicArn': emailSnsTopic
                  }

                  try:
                      response = sns.publish(**params)
                      logger.debug('MessageId: ' + response['MessageId'])
                      return None

                  except Exception as e:
                      logger.error('Error publishing to SNS topic: ' + str(e))
                      logger.error(traceback.format_exc())
                      raise e
              else:
                  logger.error('Skipping SNS publishing as emailMessage or emailSubject or emailSnsTopic is missing.')
                  return None          
          
  LambdaSendNotificationRole:
    Type: AWS::IAM::Role
    Condition: ShouldDeployDefaultNotificationFlowResources
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Action:
              - 'sts:AssumeRole'
            Effect: 'Allow'
            Principal:
              Service:
                - 'lambda.amazonaws.com'
      Policies:
        - PolicyName: RolePolicy 
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                Resource: !Sub "arn:${AWS::Partition}:logs:${AWS::Region}:${AWS::AccountId}:*"
              - Effect: Allow
                Action:
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: !Sub "arn:${AWS::Partition}:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/lambda/${AWS::StackName}-CADRI-send-notification:*"  
              - Effect: Allow
                Action: sns:Publish
                Resource: !Ref NotificationSNSTopic
  
  EventBridgeRuleSendNotification:
    Type: AWS::Events::Rule
    Condition: ShouldDeployDefaultNotificationFlowResources
    Properties:
      Name: !Sub ${AWS::StackName}-CADRI-EventBridge-notification-rule
      Description: EventBridge rule for the default notification flow. This rule will invoke the lambda function that will send the notification email
      EventBusName: !Ref EventBridgeBus
      EventPattern:
        source:
          - "custom.cadri"
      Targets:
        - Arn: !GetAtt LambdaSendNotificationFunction.Arn
          Id: tragetNotificationLambdaFunction

  LambdaSendNotificationInvoke:
    Type: AWS::Lambda::Permission
    Condition: ShouldDeployDefaultNotificationFlowResources
    Properties:
      Action: 'lambda:InvokeFunction'
      FunctionName: !GetAtt 'LambdaSendNotificationFunction.Arn'
      Principal: 'events.amazonaws.com'
      SourceArn: !GetAtt 'EventBridgeRuleSendNotification.Arn'

Outputs:
  SNSTopicArn:
    Description: ARN of the SNS topic to be used as part of the alert subscription for AWS Cost Anomaly Detection.
    Value: !Ref EventsSNSTopic